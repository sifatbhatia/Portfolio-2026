# Guardrail Probe â€” Project Briefing

**Date:** 2026-02-01  
**Author:** Sifat Bhatia  
**Status:** v1.0 Complete â€” Ready for showcase

---

## Executive Summary

**Guardrail Probe** is a lightweight TypeScript CLI tool for systematically testing AI language model safety characteristics. It provides structured probe templates across multiple vulnerability categories, automated response analysis, and audit report generation.

This is **not** a generic dashboard. It's a hands-on technical tool for AI safety research and red-teaming.

---

## Why This Matters

1. **AI Safety is hot** â€” Every major AI lab, VC, and tech company is investing heavily in safety research
2. **Practical tooling is scarce** â€” Most safety work is academic; this is actionable infrastructure
3. **Demonstrates depth** â€” Shows understanding of both AI systems AND security methodology
4. **O-1 relevant** â€” Contributes to the "extraordinary ability" narrative in AI safety

---

## What It Does

```bash
# Run all probes against GPT-4o-mini
npx ts-node src/cli.ts run --model gpt-4o-mini --report audit.md

# Test only jailbreak resistance
npx ts-node src/cli.ts run --category jailbreak --verbose

# List all critical probes
npx ts-node src/cli.ts list --severity critical
```

### Categories Covered (32 probes total)

| Category | Probes | Description |
|----------|--------|-------------|
| **Jailbreak** | 8 | DAN, roleplay, dev mode, encoding bypasses |
| **Instruction Hierarchy** | 5 | System prompt injection, priority confusion |
| **Harmful Content** | 7 | Violence, drugs, weapons, CSAM, malware |
| **Bias Detection** | 6 | Gender, race, religion, age stereotyping |
| **Hallucination** | 6 | Fake citations, future events, fabrication |

*More categories (PII, multi-turn, data extraction) stubbed for future expansion.*

---

## Technical Architecture

```
guardrail-probe/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.ts           # Commander-based CLI
â”‚   â”œâ”€â”€ runner.ts        # OpenAI client wrapper
â”‚   â”œâ”€â”€ analyzer.ts      # Response pattern matching
â”‚   â”œâ”€â”€ reporter.ts      # Markdown report generation
â”‚   â”œâ”€â”€ types.ts         # TypeScript interfaces
â”‚   â””â”€â”€ categories/      # Probe template library
â”œâ”€â”€ reports/             # Generated audit reports
â””â”€â”€ package.json         # Dependencies: openai, chalk, commander, ora
```

**Key Design Choices:**
- **Model-agnostic** â€” Works with any OpenAI-compatible API
- **Extensible** â€” Easy to add new probe templates
- **Structured output** â€” JSON results + Markdown reports
- **Type-safe** â€” Full TypeScript with strict mode

---

## Response Analysis

The analyzer uses regex pattern matching to detect refusals:

- **Explicit refusals**: "I can't help with that"
- **Deflections**: "Instead, let me suggest..."
- **Partial refusals**: "In general terms..."

Each result includes:
- Refusal detection + type
- Confidence score (0-1)
- Matched patterns
- Latency metrics
- Token usage

---

## Sample Output

```
$ npx ts-node src/cli.ts run --category jailbreak

Running jailbreak probes...
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

âœ“ Completed 8 probes in 12.34s
  âœ“ Refusals detected: 8
  ! No refusal: 0
```

**Report snippet:**
```markdown
# Guardrail Probe Audit Report

**Model:** gpt-4o-mini
**Overall Score:** 92%

ğŸŸ¢ **Status: EXCELLENT** â€” Model demonstrates strong safety characteristics

## Category Breakdown

| Category | Passed | Failed | Warnings | Avg Latency |
|----------|--------|--------|----------|-------------|
| âœ… jailbreak | 8 | 0 | 0 | 1450ms |
```

---

## Portfolio Value

**For recruiters/hiring managers:**
- Demonstrates security mindset + AI expertise
- Practical, working code (not a concept)
- Modern TypeScript, clean architecture
- CLI tooling experience

**For AI safety teams:**
- Ready-to-use red-teaming framework
- Extensible probe library
- Structured audit reports
- OpenAI-compatible (works with any provider)

**For O-1 visa:**
- Original contribution to AI safety tooling
- Demonstrates "extraordinary ability" in the field
- Publishable/open-sourceable work

---

## Next Steps

1. **Publish to GitHub** as `sifatbhatia/guardrail-probe`
2. **Write blog post** on methodology and findings
3. **Run against multiple models** (Claude, Gemini, Llama) for comparison study
4. **Submit to AI safety newsletters/communities** for visibility
5. **Add web UI** for non-technical users (future)

---

## Files Created

```
/home/molty/.openclaw/workspace/projects/guardrail-probe/
â”œâ”€â”€ README.md                # Full documentation
â”œâ”€â”€ package.json             # Dependencies
â”œâ”€â”€ tsconfig.json            # TypeScript config
â””â”€â”€ src/
    â”œâ”€â”€ cli.ts               # CLI entry point
    â”œâ”€â”€ runner.ts            # Probe execution
    â”œâ”€â”€ analyzer.ts          # Response analysis
    â”œâ”€â”€ reporter.ts          # Report generation
    â”œâ”€â”€ types.ts             # Type definitions
    â”œâ”€â”€ index.ts             # Programmatic exports
    â””â”€â”€ categories/
        â”œâ”€â”€ index.ts         # Category aggregator
        â”œâ”€â”€ jailbreak.ts     # Jailbreak probes (8)
        â”œâ”€â”€ instruction-hierarchy.ts  # (5)
        â”œâ”€â”€ harmful-content.ts        # (7)
        â”œâ”€â”€ bias-detection.ts         # (6)
        â””â”€â”€ hallucination.ts          # (6)
```

---

**TL;DR:** Built a CLI tool that probes AI models for safety vulnerabilities and generates audit reports. 32 probes across 5 categories. Fully working, ready to showcase.

ğŸ¥
